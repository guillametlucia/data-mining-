"""
Implements query likelihood language models with 
    Laplace smoothing, 
    Lidstone correction with epsilon = 0.1 and 
    Dirichlet smoothing with mu = 50
Ranks passages based on queries, calculates probabilities, retrieves 100 passages for each query (from 1000 candidates).
    Queries: test-queries.tsv
    Passages: candidate-passages-top1000.tsv
Saves the results to CSV files (laplace.csv, lidstone.csv and dirichlet.csv).
    Column score should report natural logratihm of probability scores
    Each row has qid,pid,score with no space between comas

"""
# Query likelihood language models: rank documents by the probabilty that the query was generated by the document model.
    # estimate a language model MD for each document in collection.
    # probabilty of generating query Q from document D is P(Q|D) = product of P(q|D) for each term q in Q.
    # eg tobacco advertising. P(tobacco|D) * P(advertising|D) = 2/65 * 3/65 (for a document with 65 terms in which tobacco appears twice and advertising appears three times)

# Laplace smoothing: add 1 to the count of each term in the document and add |V| to the denominator where |V| is the size of the vocabulary.
    # eg tobacco companies (companies happens 0 times). P(tobacco|D) * P(companies|D) = 3/(65+ V) * 1/(65+ V)

# queries: qid, terms in query
# passages: pid, terms in passage, length of passage, unique vocabu;lary size

# Lidstone correction: add a small constant (epsilon) to the count of each term in the document and add epsilon * |V| to the denominator. 
    # eg tobacco companies (companies happens 0 times). P(tobacco|D) * P(companies|D) = (3+0.1)/(65+ 0.1*V) * (0.1)/(65+ 0.1*V)

# Drichlet smoothing: add mu * P(w|C) to the count of each term in the document and add mu to the denominator.
    #  N/(N+mu) * P(w|D) + mu/(N+mu) * P(w|C) 
    # eg tobacco companies (companies happens 0 times). P(tobacco|D) * P(companies|D) = (3+50*P(companies|C))/(65+ 50) * (50*P(companies|C))/(65+ 50)

import json
import pandas as pd
import numpy as np
from task1 import process_text, text_stats

# Load inverted index and candidate passages
with open('inverted_index.json', 'r') as f:
    inverted_index = json.load(f) # tf
candidate_passages = pd.read_csv('candidate-passages-top1000.tsv', sep='\t', header=None)
test_queries = pd.read_csv('test-queries.tsv', sep='\t', header=None)

# Extract vocabulary and total vocabulary length
vocab = list(inverted_index.keys())
total_length_vocab = len(vocab)

# Prepare lists of unique query IDs and passage IDs
all_qid = test_queries[0].tolist()
all_pid = candidate_passages[1].unique().tolist()
candidate_pids_per_qid = candidate_passages.groupby(0)[1].apply(list).to_dict()

# Function to calculate term frequencies for a query
def tf(query_vocab, inverted_index, pid):
    tf_ls = []
    for term in query_vocab:  # Iterate over terms in the query_vocab
        # Check if the term is in the inverted index, and get the frequency of the term
        if term in inverted_index:
            tf = inverted_index[term].get(pid, 0)
        else:
            tf = 0  # Term not in inverted index at all
        tf_ls.append(tf)
    return np.array(tf_ls)

# Calculate collection frequencies
collection_freq = {}
for term, docs in inverted_index.items():
    collection_freq[term] = sum(docs.values()) # sum up all frequencies of the term across documents
total_terms = sum(collection_freq.values()) # total number of terms in the collection

# Smoothing parameters
mu = 50
epsilon = 0.1

# Initialize dictionaries to store probabilities
laplace_dictionary = {qid: [] for qid in all_qid}
lidstone_dictionary = {qid: [] for qid in all_qid}
dirichlet_dictionary = {qid: [] for qid in all_qid}

# Calculate probabilities for each query and passage
for qid in all_qid:
    candidate_pids = candidate_pids_per_qid[qid]
    query = test_queries[test_queries[0] == qid][1].iloc[0]
    vocab= process_text(query) # tokenize query
    for pid in candidate_pids: 
        passage = candidate_passages[candidate_passages[1] == pid][2].iloc[0]
        _, vocab_size, doc_len, _ = text_stats(passage, remove_stopwords=True) # tokenize passage
        denominator = doc_len + total_length_vocab
        tf_array = tf(vocab, inverted_index, pid)

        # Laplace smoothing
        prob_array = (tf_array+1)/denominator
        prob= np.prod(prob_array)
        laplace_dictionary[qid].append(np.log(prob))

        # Lidstone correction
        denominator_lidstone = (doc_len + epsilon*total_length_vocab)
        prob_lidstone = np.prod((tf_array+epsilon)/denominator_lidstone)
        lidstone_dictionary[qid].append(np.log(prob_lidstone))

        # Dirichlet smoothing
        coef_1 = doc_len/(doc_len+mu)
        coef_2 = mu/(doc_len+mu)
        collection_freq_array = np.array([collection_freq.get(term, 0) for term in vocab])
        prob_dirichlet = np.prod(coef_1*tf_array/doc_len + coef_2*collection_freq_array/total_terms)
        dirichlet_dictionary[qid].append(np.log(prob_dirichlet))

# Function to get top 100 scores
def sim_rank(cosine_sim_results):
    result = np.array(cosine_sim_results).argsort()[-100:][::-1]
    return result

# Get top 100 scores for each query
results_laplace = {}
results_lidstone = {}
results_dirichlet = {}
for qid in all_qid:
    results_laplace[qid] = sim_rank(laplace_dictionary[qid]) # for every qid getting top 100 score indices
    results_lidstone[qid] = sim_rank(lidstone_dictionary[qid])
    results_dirichlet[qid] = sim_rank(dirichlet_dictionary[qid])

# Save results to CSV files
def save_results(filename, results, dictionary):
    with open(filename, "w") as f:
        for qid, indices in results.items():
            candidate_pids = candidate_pids_per_qid[qid]
            for index in indices:
                pid = candidate_pids[index]
                f.write(f"{qid},{pid},{dictionary[qid][index]}\n")

save_results("laplace.csv", results_laplace, laplace_dictionary)
save_results("lidstone.csv", results_lidstone, lidstone_dictionary)
save_results("dirichlet.csv", results_dirichlet, dirichlet_dictionary)

