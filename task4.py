# Query likelihood language models

# use test-queries.tsv and candidate-passages-top1000.tsv
# implemeny query likelihood language models with 
    # Laplace smoothing
    # Lidstone correctoin with epsilon = 0.1
    # Drichlet smoothing with mu = 50
# retrieve 100 passages from within the 1000 candidate passages for each query

# no need to describe approach

# store respective outomes in files named laplace.csv, lidstone.csv and dirichlet.csv
# column score should report natural logratihm of probability scores

# each row must have qid,pid,score with no space between comas

# D11 wrtie in report
# which language model do you expect to work better
# give a few empirical exapmles based on oyur data . dont quote ids include query and talk about what passages are returned
# The examples need to be comprehensive to a reader of the report that does not have access to the data (i.e. do not refer to passage or query IDs).

# which language models are expeced ot be more similar and why. give a few empirical examples. first two .adding same probability to each word

# comment onthe value of epsilon. is it good choice, would there be abetter setting (provode range of vals) and why

# if we set mu = 5000 for dirichlet smoothing, woudl thisbe more appropriate and why


import json
import pandas as pd
import numpy as np
from task1 import process_text, text_stats

with open('inverted_index.json', 'r') as f:
    inverted_index = json.load(f) # tf

candidate_passages = pd.read_csv('candidate-passages-top1000.tsv', sep='\t', header=None)

test_queries = pd.read_csv('test-queries.tsv', sep='\t', header=None)

vocab = list(inverted_index.keys())
total_length_vocab = len(vocab)


# Query likelihood language models: rank documents by the probabilty that the query was generated by the document model.
    # estimate a language model MD for each document in collection.
    # probabilty of generating query Q from document D is P(Q|D) = product of P(q|D) for each term q in Q.
    # eg tobacco advertising. P(tobacco|D) * P(advertising|D) = 2/65 * 3/65 (for a document with 65 terms in which tobacco appears twice and advertising appears three times)

# Laplace smoothing: add 1 to the count of each term in the document and add |V| to the denominator where |V| is the size of the vocabulary.
    # eg tobacco companies (companies happens 0 times). P(tobacco|D) * P(companies|D) = 3/(65+ V) * 1/(65+ V)


# queries: qid, terms in query
# passages: pid, terms in passage, length of passage, unique vocabu;lary size

all_qid = test_queries[0].tolist()
all_pid = candidate_passages[1].unique().tolist()
candidate_pids_per_qid = candidate_passages.groupby(0)[1].apply(list).to_dict()

# def q_tf(query_vocab, inverted_index):
#     tf_ls = []
#     for term in query_vocab:  # Iterate over terms in the query_vocab
#         tf = query_vocab.get(term, 0)  # Get the frequency of the term
#         tf_ls.append(tf)
#     return np.array(tf_ls)  # Convert the list of term frequencies to a numpy array


def tf(query_vocab, inverted_index, pid):
    tf_ls = []
    for term in query_vocab:  # Iterate over terms in the query_vocab
        # Check if the term is in the inverted index, and get the frequency of the term
        if term in inverted_index:
            tf = inverted_index[term].get(pid, 0)
        else:
            tf = 0  # Term not in inverted index at all
        tf_ls.append(tf)
    return np.array(tf_ls)

collection_freq = {}
for term, docs in inverted_index.items():
    collection_freq[term] = sum(docs.values()) # sum up all frequencies of the term across documents

total_terms = sum(collection_freq.values()) # total number of terms in the collection
mu = 50
epsilon = 0.1


laplace_dictionary = {qid: [] for qid in all_qid}
lidstone_dictionary = {qid: [] for qid in all_qid}
dirichlet_dictionary = {qid: [] for qid in all_qid}

for qid in all_qid:
    candidate_pids = candidate_pids_per_qid[qid]
    query = test_queries[test_queries[0] == qid][1].iloc[0]
    vocab= process_text(query) # tokenize query
    for pid in candidate_pids: 
        passage = candidate_passages[candidate_passages[1] == pid][2].iloc[0]
        _, vocab_size, doc_len, _ = text_stats(passage, remove_stopwords=True) # tokenize passage
        denominator = doc_len + total_length_vocab
        tf_array = tf(vocab, inverted_index, pid)
        prob_array = (tf_array+1)/denominator
        prob= np.prod(prob_array)
        laplace_dictionary[qid].append(np.log(prob))
        denominator_lidstone = (doc_len + epsilon*total_length_vocab)
        prob_lidstone = np.prod((tf_array+epsilon)/denominator_lidstone)
        lidstone_dictionary[qid].append(np.log(prob_lidstone))

        coef_1 = doc_len/(doc_len+mu)
        coef_2 = mu/(doc_len+mu)
        collection_freq_array = np.array([collection_freq.get(term, 0) for term in vocab])
        prob_dirichlet = np.prod(coef_1*tf_array/doc_len + coef_2*collection_freq_array/total_terms)

        dirichlet_dictionary[qid].append(np.log(prob_dirichlet))
def sim_rank(cosine_sim_results):
    result = np.array(cosine_sim_results).argsort()[-100:][::-1]
    return result

results_laplace = {}
results_lidstone = {}
results_dirichlet = {}
for qid in all_qid:
    results_laplace[qid] = sim_rank(laplace_dictionary[qid]) # for every qid getting top 100 score indices
    results_lidstone[qid] = sim_rank(lidstone_dictionary[qid])
    results_dirichlet[qid] = sim_rank(dirichlet_dictionary[qid])


with open("laplace.csv", "w") as f:
    for qid, indices in results_laplace.items():
        candidate_pids = candidate_pids_per_qid[qid]
        for index in indices:
            pid = candidate_pids[index]
            # Write the qid, pid, and score to the file
            f.write(str(qid) + "," + str(pid) + "," + str(laplace_dictionary[qid][index]) + "\n")
with open("lidstone.csv", "w") as f:
    for qid, indices in results_lidstone.items():
        candidate_pids = candidate_pids_per_qid[qid]
        for index in indices:
            pid = candidate_pids[index]
            # Write the qid, pid, and score to the file
            f.write(str(qid) + "," + str(pid) + "," + str(lidstone_dictionary[qid][index]) + "\n")
with open("dirichlet.csv", "w") as f:
    for qid, indices in results_dirichlet.items():
        candidate_pids = candidate_pids_per_qid[qid]
        for index in indices:
            pid = candidate_pids[index]
            # Write the qid, pid, and score to the file
            f.write(str(qid) + "," + str(pid) + "," + str(dirichlet_dictionary[qid][index]) + "\n")
# D11 wrtie in report   

# Lidstone correction: add a small constant (epsilon) to the count of each term in the document and add epsilon * |V| to the denominator. 
    # eg tobacco companies (companies happens 0 times). P(tobacco|D) * P(companies|D) = (3+0.1)/(65+ 0.1*V) * (0.1)/(65+ 0.1*V)

# Drichlet smoothing: add mu * P(w|C) to the count of each term in the document and add mu to the denominator.
    #  N/(N+mu) * P(w|D) + mu/(N+mu) * P(w|C) 
    # eg tobacco companies (companies happens 0 times). P(tobacco|D) * P(companies|D) = (3+50*P(companies|C))/(65+ 50) * (50*P(companies|C))/(65+ 50)
